{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hispanic-embassy",
   "metadata": {
    "id": "hispanic-embassy"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "OguA1m2xC2R2",
   "metadata": {
    "id": "OguA1m2xC2R2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.utils.data as data_utils\n",
    "from model import Transformer,TCN,LSTMWithInputCellAttention,LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zN0wfRhAtTn7",
   "metadata": {
    "id": "zN0wfRhAtTn7"
   },
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST('./data', train=True, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "test_set = datasets.MNIST('./data', train=False, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                          ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arbitrary-liability",
   "metadata": {
    "id": "arbitrary-liability"
   },
   "outputs": [],
   "source": [
    "def mix(i,j,dataset):\n",
    "  ret = torch.zeros_like(dataset.data[i], dtype=torch.float)\n",
    "  ret[:, torch.arange(0, 28, 2)] = dataset.data[i][:, torch.arange(0, 28, 2)].float()\n",
    "  ret[:, torch.arange(0, 28, 2) + 1] = dataset.data[j][:, torch.arange(0, 28, 2) + 1].float()\n",
    "\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oo6wR3Fe9HGK",
   "metadata": {
    "id": "oo6wR3Fe9HGK"
   },
   "outputs": [],
   "source": [
    "def data_generator_random(batch_size,returnSet=False):\n",
    "    train_set = datasets.MNIST('./data', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "    test_set = datasets.MNIST('./data', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "    \n",
    "    new_train_data = []       # mixed image\n",
    "    new_train_targets = []    # even column label (0 based)\n",
    "    new_train_noise = []      # odd column label  (0 based)\n",
    "    id1 = []\n",
    "    for i in range(0,10):\n",
    "      id1.append([id for id, e in enumerate(train_set.targets) if e == i])\n",
    "    repeat = 2\n",
    "    for t in range(0,10):   # targets\n",
    "      for i in id1[t]:   \n",
    "        for num in range(repeat):\n",
    "          noise = random.randint(0, 9)\n",
    "          while noise == t:\n",
    "            noise = random.randint(0, 9)\n",
    "          noise_idx = random.sample(id1[noise], 1)[0]\n",
    "          new_train_data.append(mix(i,noise_idx,train_set))\n",
    "          new_train_targets.append(t)\n",
    "      new_train_noise.append(noise)\n",
    "    \n",
    "    new_test_data = []       # mixed image\n",
    "    new_test_targets = []    # even column label (0 based)\n",
    "    new_test_noise = []      # odd column label  (0 based)\n",
    "\n",
    "    id2 = []\n",
    "    for i in range(0,10):\n",
    "      id2.append([id for id, e in enumerate(test_set.targets) if e == i])\n",
    "    repeat = 2\n",
    "    for t in range(0,10):   # targets\n",
    "      for i in id2[t]:   \n",
    "        for num in range(repeat):\n",
    "          noise = random.randint(0, 9)\n",
    "          while noise == t:\n",
    "            noise = random.randint(0, 9)\n",
    "          noise_idx = random.sample(id2[noise], 1)[0]\n",
    "          new_test_data.append(mix(i,noise_idx, test_set))\n",
    "          new_test_targets.append(t)\n",
    "          new_test_noise.append(noise)\n",
    "    \n",
    "    new_train_data = torch.stack((new_train_data))\n",
    "    new_train_data -= new_train_data.min(1, keepdim=True)[0]\n",
    "    new_train_data /= new_train_data.max(1, keepdim=True)[0]\n",
    "    new_train_data = torch.nan_to_num(new_train_data, nan=0.0)\n",
    "\n",
    "    arr = np.array(new_train_targets)\n",
    "    t = torch.from_numpy(arr)\n",
    "    mnist_train = data_utils.TensorDataset(new_train_data,t)\n",
    "\n",
    "    new_test_data = torch.stack((new_test_data))\n",
    "    new_test_data -= new_test_data.min(1, keepdim=True)[0]\n",
    "    new_test_data /= new_test_data.max(1, keepdim=True)[0]\n",
    "    new_test_data = torch.nan_to_num(new_test_data, nan=0.0)\n",
    "\n",
    "    arr2 = np.array(new_test_targets)\n",
    "    t2 = torch.from_numpy(arr2)\n",
    "    minst_test = data_utils.TensorDataset(new_test_data,t2)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(minst_test, batch_size=batch_size)  \n",
    "    \n",
    "    if(returnSet):\n",
    "      return train_set, test_set\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-savage",
   "metadata": {
    "id": "encouraging-savage"
   },
   "source": [
    "## Define Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-latitude",
   "metadata": {
    "id": "tracked-latitude"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from typing import *\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-neighborhood",
   "metadata": {
    "id": "adult-neighborhood"
   },
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None,returnWeights=False):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    output = torch.matmul(scores, v)\n",
    "    # print(\"Scores in attention itself\",torch.sum(scores))\n",
    "    if(returnWeights):\n",
    "        return output,scores\n",
    "\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None,returnWeights=False):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate attention using function we will define next\n",
    "\n",
    "        if(returnWeights):\n",
    "            scores,weights = attention(q, k, v, self.d_k, mask, self.dropout,returnWeights=returnWeights)\n",
    "            # print(\"scores\",scores.shape,\"weights\",weights.shape)\n",
    "        else:\n",
    "            scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        # print(\"Attention output\", output.shape,torch.min(output))\n",
    "        if(returnWeights):\n",
    "            return output,weights\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-investing",
   "metadata": {
    "id": "ruled-investing"
   },
   "outputs": [],
   "source": [
    "# Fine tune d_ff, dropout\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=448, dropout = 0.2):\n",
    "        super().__init__() \n",
    "    \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-andorra",
   "metadata": {
    "id": "informational-andorra"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x,returnWeights=False):\n",
    "        if(returnWeights):\n",
    "            attenOutput,attenWeights= self.attn(x,x,x,returnWeights=returnWeights)\n",
    "        else:\n",
    "            attenOutput= self.attn(x,x,x)\n",
    "        x = x + self.dropout_1(attenOutput)\n",
    "        y = self.norm_2(x)\n",
    "        out = y + self.dropout_2(self.ff(y))\n",
    "        if(returnWeights):\n",
    "            return out,attenWeights\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-premium",
   "metadata": {
    "id": "exterior-premium"
   },
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-speed",
   "metadata": {
    "id": "electrical-speed"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,seq_len, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "#         self.pe = PositionalEncoder(input_size,seq_len, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(input_size, heads, dropout), N)\n",
    "        self.norm = Norm(input_size)\n",
    "    def forward(self, x,returnWeights=False):\n",
    "#         x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            if(i==0 and returnWeights):\n",
    "                x,weights = self.layers[i](x,returnWeights=returnWeights)\n",
    "            else:\n",
    "                x = self.layers[i](x)\n",
    "\n",
    "        if(returnWeights):\n",
    "            return self.norm(x),weights\n",
    "        else:\n",
    "            return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d15100",
   "metadata": {
    "id": "87d15100"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 100, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-trick",
   "metadata": {
    "id": "statewide-trick"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, N, heads, dropout, num_classes, time=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pe = PositionalEncoder(input_size,seq_len, dropout)\n",
    "        \n",
    "        self.encoder = Encoder(input_size,seq_len, N, heads, dropout)\n",
    "        self.out = nn.Linear(input_size, num_classes) \n",
    "        self.tempmaxpool = nn.MaxPool1d(time)\n",
    "        \n",
    "    def forward(self, src, returnWeights=False):\n",
    "        src = self.pe(src)\n",
    "        src= src.transpose(1, 2)\n",
    "        if(returnWeights):\n",
    "            e_outputs,weights = self.encoder(src,returnWeights=returnWeights)\n",
    "        else:\n",
    "            e_outputs = self.encoder(src)\n",
    "\n",
    "        e_outputs=self.tempmaxpool(e_outputs.transpose(1, 2)).squeeze(-1)\n",
    "        output = self.out(e_outputs)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        if(returnWeights):\n",
    "            return output,weights\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-alpha",
   "metadata": {
    "id": "saving-alpha"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-rapid",
   "metadata": {
    "id": "robust-rapid"
   },
   "source": [
    "Reference https://github.com/ayaabdelsalam91/TS-Interpretability-Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-huntington",
   "metadata": {
    "id": "minimal-huntington"
   },
   "source": [
    "Tunable Parameters:\n",
    "1. \\# of encoder layer\n",
    "2. \\# of hidden unit in positional encoding\n",
    "3. \\# of hidden unit in encoder MLP\n",
    "4. Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-assurance",
   "metadata": {
    "id": "honest-assurance"
   },
   "source": [
    "### Try deep network with higher dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adult-therapy",
   "metadata": {
    "id": "adult-therapy"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acknowledged-drama",
   "metadata": {
    "id": "acknowledged-drama"
   },
   "outputs": [],
   "source": [
    "def train(ep,model,train_loader,optimizer, ls):\n",
    "\n",
    "  train_loss = 0\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "\n",
    "      data = data.view(-1, 28, 28)\n",
    "      data, target = Variable(data), Variable(target)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data)\n",
    "      loss = ls(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss\n",
    "      if batch_idx > 0 and batch_idx % 100 == 0:\n",
    "          message = ('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "              ep, batch_idx * 128, len(train_loader.dataset),\n",
    "              100. * batch_idx / len(train_loader), train_loss.item()/100))\n",
    "          print(message)\n",
    "          train_loss = 0\n",
    "  return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-access",
   "metadata": {
    "id": "accurate-access"
   },
   "outputs": [],
   "source": [
    "def test(model,test_loader,ls):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  Acc=0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        data = data.view(-1, 28, 28)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += ls(output, target)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    Acc = 100. * correct / len(test_loader.dataset)\n",
    "    message = ('\\nTest set: Average loss: {:.10f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),Acc))\n",
    "    print(message)\n",
    "\n",
    "    return test_loss,Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa7ae4",
   "metadata": {
    "id": "96fa7ae4"
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = data_generator_random_0_2(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-possibility",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lightweight-possibility",
    "outputId": "797b77c5-d951-4a98-a1ae-cac34be34a5d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12800/118810 (11%)]\tLoss: 0.525238\n",
      "Train Epoch: 1 [25600/118810 (22%)]\tLoss: 0.407589\n",
      "Train Epoch: 1 [38400/118810 (32%)]\tLoss: 0.374858\n",
      "Train Epoch: 1 [51200/118810 (43%)]\tLoss: 0.365789\n",
      "Train Epoch: 1 [64000/118810 (54%)]\tLoss: 0.359361\n",
      "Train Epoch: 1 [76800/118810 (65%)]\tLoss: 0.348769\n",
      "Train Epoch: 1 [89600/118810 (75%)]\tLoss: 0.350855\n",
      "Train Epoch: 1 [102400/118810 (86%)]\tLoss: 0.345109\n",
      "Train Epoch: 1 [115200/118810 (97%)]\tLoss: 0.342503\n",
      "\n",
      "Test set: Average loss: 0.0025730222, Accuracy: 19839/20120 (98.60%)\n",
      "\n",
      "Train Epoch: 2 [12800/118810 (11%)]\tLoss: 0.345887\n",
      "Train Epoch: 2 [25600/118810 (22%)]\tLoss: 0.337004\n",
      "Train Epoch: 2 [38400/118810 (32%)]\tLoss: 0.339536\n",
      "Train Epoch: 2 [51200/118810 (43%)]\tLoss: 0.336598\n",
      "Train Epoch: 2 [64000/118810 (54%)]\tLoss: 0.337952\n",
      "Train Epoch: 2 [76800/118810 (65%)]\tLoss: 0.336491\n",
      "Train Epoch: 2 [89600/118810 (75%)]\tLoss: 0.338842\n",
      "Train Epoch: 2 [102400/118810 (86%)]\tLoss: 0.333091\n",
      "Train Epoch: 2 [115200/118810 (97%)]\tLoss: 0.332555\n",
      "\n",
      "Test set: Average loss: 0.0025443695, Accuracy: 19911/20120 (98.96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ep = 2\n",
    "dropout = 0.1\n",
    "layer = 12\n",
    "decay = 0.001\n",
    "lr = 0.0005\n",
    "\n",
    "model = Transformer(28, 28, layer, 4, dropout, 10,time=28)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = decay)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, ep+1):\n",
    "  model,optimizer = train(epoch,model,train_loader,optimizer,loss)\n",
    "  test_loss,test_acc = test(model,test_loader,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MUY2Z8ZzeKuw",
   "metadata": {
    "id": "MUY2Z8ZzeKuw"
   },
   "source": [
    "## PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "XXGOMlMN9UXI",
   "metadata": {
    "id": "XXGOMlMN9UXI"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4FR7TJM68UpR",
   "metadata": {
    "id": "4FR7TJM68UpR"
   },
   "outputs": [],
   "source": [
    "from model import Transformer,TCN,LSTMWithInputCellAttention,LSTM\n",
    "\n",
    "model = torch.load(open('./m_model_Transformer_NumClasses_10.pt', 'rb'), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "_7A21zLhAZO-",
   "metadata": {
    "id": "_7A21zLhAZO-"
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = data_generator_random(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "S5O1LTcZEzKA",
   "metadata": {
    "id": "S5O1LTcZEzKA"
   },
   "outputs": [],
   "source": [
    "def attack(model, images, labels, device, noise = -1, att_col=[], att_row=[], cols=[],\n",
    "           eps = 0.05, alpha= 2/255, steps=40):\n",
    "    if len(images.shape) < 3:\n",
    "        images = images.unsqueeze(0)\n",
    "        labels = labels.unsqueeze(0)\n",
    "        if noise != -1:\n",
    "            noise = noise.unsqueeze(0)\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.clone().detach().to(device)\n",
    "\n",
    "    num_images = images.shape[0]\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = nn.NLLLoss()\n",
    "\n",
    "    # Calculate orginal loss\n",
    "    old_loss = loss(outputs, labels)\n",
    "\n",
    "    adv_images = images.clone().detach()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        adv_images.requires_grad = True\n",
    "        outputs = model(adv_images)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "            \n",
    "        if noise == -1:\n",
    "            cost = loss(outputs, labels)\n",
    "        else:\n",
    "            cost = -loss(outputs, noise)\n",
    "\n",
    "        # Update adversarial images\n",
    "        full_grad = torch.autograd.grad(cost, adv_images,\n",
    "                                        retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        grad = torch.zeros_like(full_grad)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            if len(att_row) == 0:  # Attacking att_col\n",
    "                for c in att_col:\n",
    "                    grad[i, :, c] = full_grad[i, :, c]\n",
    "            else:  # Attacking att_col[0], att_row = [...]\n",
    "                grad = full_grad.clone().detach()\n",
    "                for c in range(28):\n",
    "                    if c not in cols:\n",
    "                        grad[i,:,c] = 0\n",
    "                for r in att_row:\n",
    "                    grad[i,r,att_col[0]] = 0\n",
    "\n",
    "        adv_images = adv_images.detach() + alpha * grad.sign()\n",
    "        delta = torch.clamp(adv_images - images, min=-eps, max=eps)\n",
    "        adv_images = torch.clamp(images + delta, min=0, max=1).detach()\n",
    "\n",
    "    outputs = model(adv_images)\n",
    "    new_loss = loss(outputs, labels)\n",
    "\n",
    "    if len(att_row) == 0:\n",
    "        return adv_images, (new_loss - old_loss)/(28*len(att_col))\n",
    "    else:\n",
    "        return adv_images, (new_loss - old_loss)/(28*len(cols)-len(att_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a110b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "878a7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "yvmcjqtgs7-u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "yvmcjqtgs7-u",
    "outputId": "eec4327e-1f34-4983-9bbe-39208b531ab1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.title(f\"Label(Even):{labels[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ae28243a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7346e+01, -1.8540e+01, -1.9702e+01, -1.2556e+01, -1.1539e+01,\n",
       "         -1.5740e+01, -1.9408e+01, -1.1781e+01, -1.5580e+01, -2.1219e-05]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(test_img.unsqueeze(0).to(device))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nrL6dqMJcfZx",
   "metadata": {
    "id": "nrL6dqMJcfZx"
   },
   "outputs": [],
   "source": [
    "def find_important_independent_colunms(model, image, label, noise, device, n_cols):\n",
    "    scores = []\n",
    "    for col in range(28):\n",
    "        _, score = attack(model, image, label, device, noise, att_col=[col])\n",
    "        scores.append((score.item(), col))\n",
    "    scores.sort(reverse=True)\n",
    "    cols = [c for _, c in scores[:n_cols]]\n",
    "    best_scores = [s for s, _ in scores[:n_cols]]\n",
    "    return cols, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "Lg2cAc_61HGB",
   "metadata": {
    "id": "Lg2cAc_61HGB"
   },
   "outputs": [],
   "source": [
    "def find_important_joint_colunms(model, image, label, noise, device, n_cols):\n",
    "    available_cols = [i for i in range(28)]\n",
    "    chosen_cols = []\n",
    "    prev_best_score = float(\"-inf\")\n",
    "    scores = []\n",
    "    label = torch.as_tensor(label).to(device)\n",
    "    noise = torch.as_tensor(noise).to(device)\n",
    "    for _ in range(n_cols):\n",
    "        best_col = None\n",
    "        best_score = float(\"-inf\")\n",
    "        for col in available_cols:\n",
    "            _, score = attack(model, image, label, device, noise, att_col=chosen_cols+[col])\n",
    "            score = score.item()\n",
    "            if score > best_score:\n",
    "                best_col = col\n",
    "                best_score = score\n",
    "        chosen_cols.append(best_col)\n",
    "        available_cols.remove(best_col)\n",
    "        if prev_best_score == float(\"-inf\"):\n",
    "            scores.append(best_score)\n",
    "        else:\n",
    "            scores.append(best_score - prev_best_score)\n",
    "        prev_best_score = best_score\n",
    "\n",
    "    return chosen_cols, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "myUm2UUwYyTR",
   "metadata": {
    "id": "myUm2UUwYyTR"
   },
   "outputs": [],
   "source": [
    "def find_important_rows(model, image, label, noise, device, cols, n_row):\n",
    "    label = torch.as_tensor(label).to(device)\n",
    "    noise = torch.as_tensor(noise).to(device)\n",
    "    important_rows = []\n",
    "    for col in cols:\n",
    "        scores = []\n",
    "        for row in range(28):\n",
    "            _, score = attack(model, image, label, device, noise, att_col=[col], att_row=[row])\n",
    "            scores.append((score.item(), row))\n",
    "        scores.sort(reverse=True)\n",
    "        best_rows = [r for _, r in scores[:n_row]]\n",
    "        important_rows.append(best_rows)\n",
    "\n",
    "    return important_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "P5Glm05oo9Rr",
   "metadata": {
    "id": "P5Glm05oo9Rr"
   },
   "outputs": [],
   "source": [
    "def find_important_joint_rows(model, image, label, noise, device, cols, n_rows):\n",
    "    label = torch.as_tensor(label).to(device)\n",
    "    noise = torch.as_tensor(noise).to(device)\n",
    "    important_rows = []\n",
    "    all_scores = []\n",
    "    for col in cols:\n",
    "        available_rows = [i for i in range(28)]\n",
    "        chosen_rows = []\n",
    "        prev_best_score = float('inf')\n",
    "        scores = []\n",
    "        for n_row in range(n_rows):\n",
    "            best_row = None\n",
    "            best_score = float('inf')\n",
    "            for row in available_rows:\n",
    "                _, score = attack(model, image, label, device, noise, att_col=[col], att_row = chosen_rows+[row], cols=cols)\n",
    "                score = score.item()\n",
    "                if score < best_score:\n",
    "                    best_row = row\n",
    "                    best_score = score\n",
    "            chosen_rows.append(best_row)\n",
    "            available_rows.remove(best_row)\n",
    "            if prev_best_score == float(\"inf\"):\n",
    "                scores.append(best_score)\n",
    "            else:\n",
    "                scores.append(prev_best_score - best_score)\n",
    "            prev_best_score = best_score\n",
    "        important_rows.append(chosen_rows)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    return all_scores, important_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7870000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pixels_from_column(model, image, label, noise, device, cols, n_pixels):\n",
    "    label = torch.as_tensor(label).to(device)\n",
    "    noise = torch.as_tensor(noise).to(device)\n",
    "    prev_best_score = float(\"inf\")\n",
    "    pixels = []\n",
    "    for _ in range(n_pixels):\n",
    "        best_score = float(\"inf\")\n",
    "        best_pixel = None\n",
    "        for row in range(28):\n",
    "            for col in cols:\n",
    "                tup = (row,col)\n",
    "                if tup not in pixels:\n",
    "                    _, score = attack_pixel(model, image, label, device, noise, cols = cols, pixels=pixels+[tup])\n",
    "                    if score < best_score:\n",
    "                        best_score = score\n",
    "                        best_pixel = tup\n",
    "        pixels.append(best_pixel)\n",
    "        prev_best_score = best_score\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "Hg_Dn5RY8Iai",
   "metadata": {
    "id": "Hg_Dn5RY8Iai"
   },
   "outputs": [],
   "source": [
    "cols, score = find_important_joint_colunms(model, test_img, 9, 6, device, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "Bx49MMu58Q1x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx49MMu58Q1x",
    "outputId": "60465557-9059-49e9-ac14-59fac29c0296"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([16, 18, 14, 21, 11, 20, 15, 10],\n",
       " [5.662276407747413e-07,\n",
       "  4.980867061021854e-07,\n",
       "  8.91365016286727e-06,\n",
       "  5.020971002522856e-05,\n",
       "  8.417852950515226e-05,\n",
       "  0.006356006459100172,\n",
       "  0.015264488756656647,\n",
       "  0.012163525447249413])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fATwSHdMqFoN",
   "metadata": {
    "id": "fATwSHdMqFoN"
   },
   "outputs": [],
   "source": [
    "s, r = find_important_joint_rows(model, test_img, 9, 6, device, [16, 18, 14, 21, 11, 20, 15, 10], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "EpZY11ufhY68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpZY11ufhY68",
    "outputId": "750efac6-7b6d-4702-b599-e3918cd4cda8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 16, 12, 23, 2, 25, 17],\n",
       " [1, 3, 18, 19, 16, 22, 7, 20],\n",
       " [26, 6, 5, 12, 2, 24, 0, 23],\n",
       " [3, 27, 23, 10, 0, 24, 7, 26],\n",
       " [0, 26, 19, 22, 24, 5, 16, 23],\n",
       " [1, 26, 8, 14, 21, 0, 3, 5],\n",
       " [0, 20, 5, 6, 9, 17, 24, 25],\n",
       " [13, 20, 27, 10, 12, 25, 18, 5]]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3Xqotc2le3hL",
   "metadata": {
    "id": "3Xqotc2le3hL"
   },
   "outputs": [],
   "source": [
    "def visualize_attack(cols, rows, image, label, noise):\n",
    "  x = []\n",
    "  y = []\n",
    "  for i in range(len(cols)):\n",
    "      col = cols[i]\n",
    "      row = rows[i]\n",
    "      for r in row:\n",
    "          x.append(col)\n",
    "          y.append(r)\n",
    "  \n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "  ax1.imshow(image, cmap='gray')\n",
    "  ax1.set_title('Data Image, Label: {}, Noise:{}'.format(label, noise))\n",
    "  ax2.scatter(x, y, c='red', marker='s')\n",
    "  ax2.imshow(image, cmap='gray')\n",
    "  ax2.set_title('Important features')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "zLSdZdqpXe_Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "zLSdZdqpXe_Y",
    "outputId": "f7a714ee-a85e-460f-cf59-e59c44315271"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_attack(cols, r, test_img, 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab6507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "honest-assurance"
   ],
   "name": "dataloader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
